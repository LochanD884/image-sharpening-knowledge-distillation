{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "I89qIzhj8n_V",
        "HPpCuKN89CEr",
        "Bm0tSxJB9iag",
        "qbNlfExU-CZZ",
        "JnhO8zUp-PvG",
        "pNiXHuWLVT3U"
      ],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# **Image Sharpeing using Knowledge Distillation**"
      ],
      "metadata": {
        "id": "2dpYtVDdaluV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# First Steps"
      ],
      "metadata": {
        "id": "I89qIzhj8n_V"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Uninstalling Libraries"
      ],
      "metadata": {
        "id": "2ineXavLv_ua"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RA0TYRN159M9"
      },
      "outputs": [],
      "source": [
        "!pip uninstall -y torch torchvision realesrgan piq basicsr"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Installing Libraries"
      ],
      "metadata": {
        "id": "JeGQuP2zwJsg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install torch==2.0.1 torchvision==0.15.2 basicsr==1.4.2 numpy==1.26.4 --force-reinstall --no-cache-dir"
      ],
      "metadata": {
        "collapsed": true,
        "id": "92fG1YqBObHo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Checking Versions"
      ],
      "metadata": {
        "id": "V6zRQjVKwM-R"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torchvision\n",
        "import numpy as np\n",
        "\n",
        "print(\"Torch:\", torch.__version__)\n",
        "print(\"Torchvision:\", torchvision.__version__)\n",
        "print(\"Numpy:\", np.__version__)"
      ],
      "metadata": {
        "id": "F5bdPC4SNCqY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Connecting to CUDA"
      ],
      "metadata": {
        "id": "_HICECQFwPtp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print(f\"‚úÖ Using device: {device}\")"
      ],
      "metadata": {
        "id": "CYGkR73HndzM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Cloning Real-ESRGAN Model"
      ],
      "metadata": {
        "id": "n8jcKxWHwVqA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!git clone https://github.com/xinntao/Real-ESRGAN.git\n",
        "%cd Real-ESRGAN\n",
        "!pip install facexlib gfpgan\n",
        "!pip install -r requirements.txt\n",
        "!python setup.py develop\n",
        "!pip install piq"
      ],
      "metadata": {
        "id": "IIJ39jCJQjIw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Importing Libraries"
      ],
      "metadata": {
        "id": "wGJrDGzswkh_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import torchvision.transforms as T\n",
        "from torchvision.transforms import ToTensor\n",
        "from PIL import Image\n",
        "import os, cv2\n",
        "from tqdm import tqdm\n",
        "import matplotlib.pyplot as plt\n",
        "import torchvision.models as models\n",
        "import torch.nn.functional as F"
      ],
      "metadata": {
        "id": "KqwMEeJD6D8a"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Perceptual Loss"
      ],
      "metadata": {
        "id": "iK_0vwRq-4tr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ‚úÖ Load VGG19 for perceptual loss\n",
        "vgg = models.vgg19(weights=models.VGG19_Weights.DEFAULT).features[:16].to(device).eval()\n",
        "\n",
        "for param in vgg.parameters():\n",
        "    param.requires_grad = False\n",
        "\n",
        "def perceptual_loss(x, y):\n",
        "    return F.l1_loss(vgg(x), vgg(y))"
      ],
      "metadata": {
        "id": "y5FTiPRT-4hD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Mounting Drive"
      ],
      "metadata": {
        "id": "aNgKPCTM858Y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive', force_remount=True)"
      ],
      "metadata": {
        "id": "HcSF-wUx6DWc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Dataset"
      ],
      "metadata": {
        "id": "HPpCuKN89CEr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Installing Dataset"
      ],
      "metadata": {
        "id": "zs54jO1J9XB6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!mkdir -p /content/DIV2K\n",
        "!wget -c https://data.vision.ee.ethz.ch/cvl/DIV2K/DIV2K_train_HR.zip -P /content/DIV2K\n",
        "!unzip -q /content/DIV2K/DIV2K_train_HR.zip -d /content/"
      ],
      "metadata": {
        "id": "VOvlHf82E-C_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Prepare Dataset"
      ],
      "metadata": {
        "id": "K2Cjx0x19azQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "sharp_dir = \"/content/DIV2K_train_HR\"\n",
        "degraded_dir = \"/content/drive/MyDrive/intel/DIV2K/degraded\"\n",
        "os.makedirs(degraded_dir, exist_ok=True)"
      ],
      "metadata": {
        "id": "oWTRPSuYK4mx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Degrade Images"
      ],
      "metadata": {
        "id": "DteB05qpxWUf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def degrade_image(img):\n",
        "    img = cv2.resize(img, (128, 128), interpolation=cv2.INTER_CUBIC)\n",
        "    img = cv2.resize(img, (256, 256), interpolation=cv2.INTER_CUBIC)\n",
        "    noise = np.random.normal(0, 2, img.shape)\n",
        "    img = np.clip(img + noise, 0, 255).astype(np.uint8)\n",
        "    return img\n",
        "\n",
        "for fname in tqdm(os.listdir(sharp_dir)):\n",
        "    img_path = os.path.join(sharp_dir, fname)\n",
        "    img = cv2.imread(img_path)\n",
        "    if img is None:\n",
        "        continue\n",
        "    degraded = degrade_image(img)\n",
        "    cv2.imwrite(os.path.join(degraded_dir, fname), degraded)"
      ],
      "metadata": {
        "id": "4bqXf5NRBs5x"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Get only filenames that exist in both folders\n",
        "sharp_names = set(os.listdir(sharp_dir))\n",
        "degraded_names = set(os.listdir(degraded_dir))\n",
        "common_names = sorted(list(sharp_names & degraded_names))\n",
        "\n",
        "print(f\"‚úÖ Matched files: {len(common_names)} found in BOTH folders\")"
      ],
      "metadata": {
        "id": "AdcUsB0aKG6L"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Sharpen Images"
      ],
      "metadata": {
        "id": "i-FCBwWw9QLA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class SharpeningDataset(Dataset):\n",
        "    def __init__(self, degraded_dir, sharp_dir, image_names, transform=None):\n",
        "        self.degraded_dir = degraded_dir\n",
        "        self.sharp_dir = sharp_dir\n",
        "        self.image_names = image_names\n",
        "        self.transform = transform or T.ToTensor()\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.image_names)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        name = self.image_names[idx]\n",
        "        degraded = Image.open(os.path.join(self.degraded_dir, name)).convert(\"RGB\")\n",
        "        sharp = Image.open(os.path.join(self.sharp_dir, name)).convert(\"RGB\")\n",
        "        degraded = self.transform(degraded)\n",
        "        sharp = self.transform(sharp)\n",
        "        return degraded, sharp"
      ],
      "metadata": {
        "id": "m_3mBiPA6NmD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Transform the Pipeline"
      ],
      "metadata": {
        "id": "AAL7DgCIxgnH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "transform = T.Compose([\n",
        "    T.RandomCrop(128),\n",
        "    T.ToTensor()\n",
        "])"
      ],
      "metadata": {
        "id": "5gCOzTrh6XkK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Loading Data"
      ],
      "metadata": {
        "id": "-TTIctWbxngH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "all_imgs = os.listdir(degraded_dir)\n",
        "train_set = SharpeningDataset(degraded_dir, sharp_dir, all_imgs, transform)\n",
        "train_loader = DataLoader(train_set, batch_size=4, shuffle=True)\n",
        "\n",
        "print(f\"‚úÖ Degraded images: {len(all_imgs)} ready for training.\")"
      ],
      "metadata": {
        "id": "MtTeFDPixQif"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Teacher Model"
      ],
      "metadata": {
        "id": "Bm0tSxJB9iag"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Download Anime6B Model"
      ],
      "metadata": {
        "id": "Li_zhryNyaO_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!wget -q https://github.com/xinntao/Real-ESRGAN/releases/download/v0.2.2.4/RealESRGAN_x4plus_anime_6B.pth -P experiments/pretrained_models"
      ],
      "metadata": {
        "id": "voIAi0feFf-7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Import Necessities"
      ],
      "metadata": {
        "id": "qIkq-4kMygKf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from basicsr.archs.rrdbnet_arch import RRDBNet\n",
        "from realesrgan import RealESRGANer"
      ],
      "metadata": {
        "id": "872DEi9_VxZO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Define Model"
      ],
      "metadata": {
        "id": "7Rj07uzF94AX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model = RRDBNet(\n",
        "    num_in_ch=3,\n",
        "    num_out_ch=3,\n",
        "    num_feat=64,\n",
        "    num_block=6,     # Anime 6B uses 6 RRDB blocks\n",
        "    num_grow_ch=32\n",
        ")"
      ],
      "metadata": {
        "id": "taM7_GBW6rKS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Load Model"
      ],
      "metadata": {
        "id": "28vYVhSly_6d"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ‚úÖ Load the RealESRGANer with RRDBNet\n",
        "teacher = RealESRGANer(\n",
        "    scale=4,\n",
        "    model_path='experiments/pretrained_models/RealESRGAN_x4plus_anime_6B.pth',\n",
        "    model=model,\n",
        "    tile=0,\n",
        "    tile_pad=10,\n",
        "    pre_pad=0,\n",
        "    half=True  # Use FP16 if supported\n",
        ")\n",
        "\n",
        "print(\"‚úÖ RealESRGAN Anime 6B Teacher loaded.\")"
      ],
      "metadata": {
        "id": "kmuSfIpJyUPH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Freeze Teacher"
      ],
      "metadata": {
        "id": "zA8WOi1qHe9Z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "teacher.model.eval()\n",
        "for p in teacher.model.parameters():\n",
        "    p.requires_grad = False\n",
        "\n",
        "print(\"‚úÖ Teacher model frozen & ready.\")"
      ],
      "metadata": {
        "id": "6KswOE_BHeFy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Student Model"
      ],
      "metadata": {
        "id": "qbNlfExU-CZZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Student Model"
      ],
      "metadata": {
        "id": "5RN8hVTmEiVV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class StudentCNN(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.body = nn.Sequential(\n",
        "            nn.Conv2d(3, 32, 3, padding=1),\n",
        "            nn.ReLU(),\n",
        "            nn.Conv2d(32, 32, 3, padding=1),\n",
        "            nn.ReLU(),\n",
        "            nn.Conv2d(32, 48, 3, padding=1),  # to prep for pixel shuffle\n",
        "            nn.PixelShuffle(2),               # upscale x2\n",
        "            nn.Conv2d(12, 48, 3, padding=1),  # channels = 12 after shuffle\n",
        "            nn.PixelShuffle(2),               # upscale x2 again ‚Üí total x4\n",
        "            nn.Conv2d(12, 3, 3, padding=1)    # final RGB output\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.body(x)\n",
        "\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "student = StudentCNN().to(device)\n",
        "print(\"‚úÖ Student model created.\")"
      ],
      "metadata": {
        "id": "Bdg46wI-pTPn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Loading existing weights"
      ],
      "metadata": {
        "id": "7VTMGCHb1Tgz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "student_path = \"/content/drive/MyDrive/intel/student_model_1.pth\"\n",
        "\n",
        "if os.path.exists(student_path):\n",
        "    try:\n",
        "        student.load_state_dict(torch.load(student_path), strict=False)\n",
        "        print(f\"‚úÖ Loaded existing student weights from {student_path} (non-strict)\")\n",
        "    except Exception as e:\n",
        "        print(f\"‚ùå Error loading state dict: {e}\")\n",
        "        print(\"‚ÑπÔ∏è Starting training with fresh weights.\")\n",
        "else:\n",
        "    print(\"‚ÑπÔ∏è No previous student weights found. Starting fresh.\")"
      ],
      "metadata": {
        "id": "aT0QDwoE1QHu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Training"
      ],
      "metadata": {
        "id": "uVy6Y4w4-HH_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.nn.functional as F\n",
        "from torchvision.models import vgg16\n",
        "from torchvision.models.feature_extraction import create_feature_extractor\n",
        "\n",
        "# ‚úÖ Setup your VGG perceptual extractor ONCE outside the loop\n",
        "vgg = vgg16(pretrained=True).features[:16].eval().to(device)\n",
        "for param in vgg.parameters():\n",
        "    param.requires_grad = False\n",
        "\n",
        "def perceptual_loss(pred, target):\n",
        "    pred_features = vgg(pred)\n",
        "    target_features = vgg(target)\n",
        "    return F.l1_loss(pred_features, target_features)\n",
        "\n",
        "criterion = nn.L1Loss()\n",
        "optimizer = optim.Adam(student.parameters(), lr=1e-4)\n",
        "\n",
        "num_epochs = 50\n",
        "lambda_perc = 0.1  # <-- you can tune this\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    student.train()\n",
        "    running_loss = 0\n",
        "\n",
        "    for degraded_imgs, _ in train_loader:\n",
        "        degraded_imgs = degraded_imgs.to(device)\n",
        "\n",
        "        teacher_out = []\n",
        "        for i in range(degraded_imgs.size(0)):\n",
        "            img_np = degraded_imgs[i].cpu().permute(1, 2, 0).numpy() * 255\n",
        "            img_np = img_np.astype(np.uint8)\n",
        "            img_bgr = cv2.cvtColor(img_np, cv2.COLOR_RGB2BGR)\n",
        "\n",
        "            output, _ = teacher.enhance(img_bgr)\n",
        "            assert isinstance(output, np.ndarray), f\"Teacher enhance output is not ndarray but {type(output)}\"\n",
        "            output_rgb = cv2.cvtColor(output, cv2.COLOR_BGR2RGB)\n",
        "            teacher_out.append(T.ToTensor()(output_rgb))\n",
        "\n",
        "        teacher_out = torch.stack(teacher_out).to(device)\n",
        "\n",
        "        student_out = student(degraded_imgs)\n",
        "\n",
        "        loss_l1 = criterion(student_out, teacher_out)\n",
        "        loss_perc = perceptual_loss(student_out, teacher_out)\n",
        "        total_loss = loss_l1 + lambda_perc * loss_perc\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        total_loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        running_loss += total_loss.item()\n",
        "\n",
        "    avg_loss = running_loss / len(train_loader)\n",
        "    print(f\"Epoch {epoch+1}/{num_epochs} | L1: {loss_l1:.4f} | Perc: {loss_perc:.4f} | Total: {total_loss:.4f} | Avg: {avg_loss:.4f}\")"
      ],
      "metadata": {
        "id": "QyMPKab-pi_x"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Saving Model"
      ],
      "metadata": {
        "id": "7qzueHpO-MN3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "student_save_path = \"/content/drive/MyDrive/intel/student_model_1.pth\"\n",
        "torch.save(student.state_dict(), student_save_path)\n",
        "print(f\"‚úÖ Student saved to {student_save_path}\")"
      ],
      "metadata": {
        "id": "XxMGZtEt7Chi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Checking Output Shape"
      ],
      "metadata": {
        "id": "HM7V7VPL0lmM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Teacher out:\", teacher_out.shape)\n",
        "print(\"Student out:\", student_out.shape)\n",
        "print(\"Degraded Input Tensor (last batch):\", degraded_imgs.shape)\n",
        "assert teacher_out.shape == student_out.shape, \"Teacher and student output shapes do not match!\""
      ],
      "metadata": {
        "id": "2B5o4teVpHk0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Mapping"
      ],
      "metadata": {
        "id": "JnhO8zUp-PvG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Evaluation Mode"
      ],
      "metadata": {
        "id": "y78RjjeP3CWC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "student.eval()\n",
        "degraded, sharp = next(iter(train_loader))\n",
        "degraded = degraded.to(device)\n",
        "sharp = sharp.to(device)\n",
        "\n",
        "print(f\"Degraded shape: {degraded.shape} | Sharp shape: {sharp.shape}\")"
      ],
      "metadata": {
        "id": "I6ojgXwB2dkG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Run Student & Unsample GT"
      ],
      "metadata": {
        "id": "ykMCpLiV3Pgr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "with torch.no_grad():\n",
        "    student_out = student(degraded)\n",
        "    gt_up = F.interpolate(\n",
        "        sharp,\n",
        "        size=(student_out.size(2), student_out.size(3)),\n",
        "        mode='bicubic',\n",
        "        align_corners=False\n",
        "    )\n",
        "\n",
        "print(f\"Student Output shape: {student_out.shape}\")\n",
        "print(f\"GT Upsampled shape: {gt_up.shape}\")"
      ],
      "metadata": {
        "id": "tdZLUQHo2hEj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Visualize"
      ],
      "metadata": {
        "id": "O__bvlD-38fK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "degraded_img = degraded[0].detach().cpu().permute(1,2,0).numpy().clip(0,1)\n",
        "student_img = student_out[0].detach().cpu().permute(1,2,0).numpy().clip(0,1)\n",
        "sharp_img = gt_up[0].detach().cpu().permute(1,2,0).numpy().clip(0,1)\n",
        "\n",
        "fig, axes = plt.subplots(1, 3, figsize=(15,5))\n",
        "axes[0].imshow(degraded_img)\n",
        "axes[0].set_title(\"Degraded Input\")\n",
        "axes[1].imshow(student_img)\n",
        "axes[1].set_title(\"Student Output\")\n",
        "axes[2].imshow(sharp_img)\n",
        "axes[2].set_title(\"Ground Truth (Upsampled)\")\n",
        "\n",
        "for ax in axes:\n",
        "    ax.axis('off')\n",
        "\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "3d554F-V21fz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Evaluation"
      ],
      "metadata": {
        "id": "pNiXHuWLVT3U"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Importing Essentials"
      ],
      "metadata": {
        "id": "YMfW19IefQ-H"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from skimage.metrics import structural_similarity as ssim\n",
        "from skimage.metrics import peak_signal_noise_ratio as psnr\n",
        "import matplotlib.pyplot as plt"
      ],
      "metadata": {
        "id": "gQcjuWeLQR4D"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Student Eval"
      ],
      "metadata": {
        "id": "sTnY8w2GfpMs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "student.eval()\n",
        "\n",
        "degraded, sharp = next(iter(train_loader))\n",
        "degraded, sharp = degraded.to(device), sharp.to(device)\n",
        "\n",
        "with torch.no_grad():\n",
        "    student_out = student(degraded)\n",
        "\n",
        "    gt_up = F.interpolate(\n",
        "        sharp,\n",
        "        size=(student_out.size(2), student_out.size(3)),\n",
        "        mode='bicubic',\n",
        "        align_corners=False\n",
        "    )"
      ],
      "metadata": {
        "id": "llnTysVdQZF5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# üëâ Convert first sample to numpy\n",
        "student_np = student_out[0].cpu().permute(1, 2, 0).numpy().clip(0, 1)\n",
        "gt_np = gt_up[0].cpu().permute(1, 2, 0).numpy().clip(0, 1)\n",
        "\n",
        "# ‚úÖ SSIM\n",
        "ssim_score = ssim(student_np, gt_np, channel_axis=2, data_range=1.0)\n",
        "print(f\"‚úÖ Student SSIM: {ssim_score:.4f}\")\n",
        "\n",
        "# ‚úÖ PSNR\n",
        "psnr_val = psnr(gt_np, student_np, data_range=1.0)\n",
        "print(f\"‚úÖ Student PSNR: {psnr_val:.2f} dB\")"
      ],
      "metadata": {
        "id": "Xz5ksr0qQbWe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "fig, axes = plt.subplots(1, 2, figsize=(12, 5))\n",
        "axes[0].imshow(student_np)\n",
        "axes[0].set_title(f\"Student Output\\nSSIM: {ssim_score:.4f}  PSNR: {psnr_val:.2f} dB\")\n",
        "axes[1].imshow(gt_np)\n",
        "axes[1].set_title(\"Ground Truth (Upsampled)\")\n",
        "for ax in axes:\n",
        "    ax.axis('off')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "COB8VMGMQeHn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Teacher Eval"
      ],
      "metadata": {
        "id": "HBVtKzhmfwi3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "teacher.model.eval()\n",
        "\n",
        "teacher_out_list = []\n",
        "for i in range(degraded.size(0)):\n",
        "    img_np = degraded[i].cpu().permute(1, 2, 0).numpy() * 255\n",
        "    img_np = img_np.astype(np.uint8)\n",
        "    img_bgr = cv2.cvtColor(img_np, cv2.COLOR_RGB2BGR)\n",
        "\n",
        "    output, _ = teacher.enhance(img_bgr)\n",
        "    output_rgb = cv2.cvtColor(output, cv2.COLOR_BGR2RGB)\n",
        "    teacher_out_list.append(T.ToTensor()(output_rgb))\n",
        "\n",
        "teacher_out = torch.stack(teacher_out_list).to(device)"
      ],
      "metadata": {
        "id": "w2LICBETQgI0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "gt_up_teacher = F.interpolate(\n",
        "    sharp,\n",
        "    size=(teacher_out.size(2), teacher_out.size(3)),\n",
        "    mode='bicubic',\n",
        "    align_corners=False\n",
        ")"
      ],
      "metadata": {
        "id": "0c-reNiNQmYW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "teacher_np = teacher_out[0].cpu().permute(1, 2, 0).numpy().clip(0, 1)\n",
        "gt_teacher_np = gt_up_teacher[0].cpu().permute(1, 2, 0).numpy().clip(0, 1)\n",
        "\n",
        "ssim_score_teacher = ssim(teacher_np, gt_teacher_np, channel_axis=2, data_range=1.0)\n",
        "psnr_teacher = psnr(gt_teacher_np, teacher_np, data_range=1.0)\n",
        "\n",
        "print(f\"‚úÖ Teacher SSIM: {ssim_score_teacher:.4f}\")\n",
        "print(f\"‚úÖ Teacher PSNR: {psnr_teacher:.2f} dB\")"
      ],
      "metadata": {
        "id": "zZ4MEy7tQpKe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "fig, axes = plt.subplots(1, 2, figsize=(12, 5))\n",
        "axes[0].imshow(teacher_np)\n",
        "axes[0].set_title(f\"Teacher Output\\nSSIM: {ssim_score_teacher:.4f}  PSNR: {psnr_teacher:.2f} dB\")\n",
        "axes[1].imshow(gt_teacher_np)\n",
        "axes[1].set_title(\"Ground Truth (Upsampled)\")\n",
        "for ax in axes:\n",
        "    ax.axis('off')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "NimLR1NwQru0"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}
